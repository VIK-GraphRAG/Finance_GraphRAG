"""
ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ìŒ
ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë†“ì€ íŒŒì¼ì´ì—ìš”!
"""

import os
import re
from typing import List, Optional, Dict, Any
from openai import AsyncOpenAI
from ollama import AsyncClient

# configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
from config import (
    OPENAI_API_KEY,
    OPENAI_BASE_URL,
    API_MODELS,
    LOCAL_MODELS,
)


# --- [1] OpenAI API í•¨ìˆ˜ë“¤ ---

async def openai_model_if(
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[Dict[str, str]] = [],
    **kwargs
) -> str:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•´ì„œ LLMì— ì§ˆë¬¸í•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Args:
        prompt: ì§ˆë¬¸ ë‚´ìš©
        system_prompt: ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì„ íƒì‚¬í•­)
        history_messages: ì´ì „ ëŒ€í™” ë‚´ìš©
        **kwargs: ì¶”ê°€ ì¸ì
        
    Returns:
        LLM ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    
    client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})
    
    response = await client.chat.completions.create(
        model=API_MODELS["llm"],
        messages=messages,
        temperature=0.0,  # 0.1 -> 0.0 (ë” ë¹ ë¥¸ ì‘ë‹µ)
        max_tokens=2000,  # 500 -> 2000 (JSON íŒŒì‹± ì—ëŸ¬ ë°©ì§€)
    )
    
    result = response.choices[0].message.content
    
    
    return result


async def openai_embedding_if(texts: List[str]) -> List[List[float]]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Args:
        texts: ë³€í™˜í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ê° í…ìŠ¤íŠ¸ë§ˆë‹¤ í•˜ë‚˜ì˜ ë²¡í„°)
    """
    client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
    
    response = await client.embeddings.create(
        model=API_MODELS["embedding"],
        input=texts
    )
    
    return [item.embedding for item in response.data]

# nano-graphragê°€ ìš”êµ¬í•˜ëŠ” embedding_dim ì†ì„± ì¶”ê°€
openai_embedding_if.embedding_dim = 1536


# --- [2] Ollama í•¨ìˆ˜ë“¤ ---

async def ollama_model_if(
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[Dict[str, str]] = [],
    **kwargs
) -> str:
    """
    Ollamaë¥¼ ì‚¬ìš©í•´ì„œ LLMì— ì§ˆë¬¸í•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Args:
        prompt: ì§ˆë¬¸ ë‚´ìš©
        system_prompt: ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì„ íƒì‚¬í•­)
        history_messages: ì´ì „ ëŒ€í™” ë‚´ìš©
        **kwargs: ì¶”ê°€ ì¸ì
        
    Returns:
        LLM ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    
    client = AsyncClient()
    
    # Detect if this is a JSON-expecting call (GraphRAG internal operations)
    is_json_request = (
        "json" in prompt.lower() or 
        "```json" in prompt.lower() or
        (system_prompt and "json" in system_prompt.lower())
    )
    
    messages = []
    if system_prompt:
        # Enhance system prompt for JSON requests
        if is_json_request:
            enhanced_system = system_prompt + "\n\nIMPORTANT: You MUST respond with valid JSON only. Do not include any explanatory text, markdown formatting, or conversational responses. Output only the raw JSON object."
            messages.append({"role": "system", "content": enhanced_system})
        else:
            messages.append({"role": "system", "content": system_prompt})
    elif is_json_request:
        # Add JSON enforcement if no system prompt exists
        messages.append({"role": "system", "content": "You are a precise JSON generator. Respond only with valid JSON. No explanations, no markdown, no conversational text."})
    
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})
    
    response = await client.chat(
        model=LOCAL_MODELS["llm"],
        messages=messages,
        format="json" if is_json_request else None  # Force JSON mode for Ollama
    )
    
    result = response['message']['content']
    
    
    # If JSON was expected, try to extract it from conversational responses
    if is_json_request and result:
        # Try to extract JSON from markdown code blocks
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', result, re.DOTALL)
        if json_match:
            result = json_match.group(1)
        # Try to find JSON object in the response
        elif not result.strip().startswith('{'):
            json_obj_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_obj_match:
                result = json_obj_match.group(0)
    
    
    return result


async def ollama_embedding_if(texts: List[str]) -> List[List[float]]:
    """
    Ollamaë¥¼ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Args:
        texts: ë³€í™˜í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ê° í…ìŠ¤íŠ¸ë§ˆë‹¤ í•˜ë‚˜ì˜ ë²¡í„°)
    """
    client = AsyncClient()
    
    embeds = []
    for text in texts:
        response = await client.embeddings(
            model=LOCAL_MODELS["embedding"],
            prompt=text
        )
        embeds.append(response['embedding'])
    
    return embeds


# Ollama embedding ì°¨ì› ì„¤ì •
ollama_embedding_if.embedding_dim = LOCAL_MODELS["embedding_dim"]


# --- [3] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---

def preprocess_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    - ë¶ˆìš©ì–´ ì œê±°
    - í•œ ê¸€ì ë‹¨ì–´ ì œê±°
    - ê¸ˆìœµ ìˆ«ì ë³´ì¡´
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        
    Returns:
        ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    stopwords = {
        "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì˜", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
        "ì€", "ëŠ”", "ì—ì„œ", "ì—ê²Œ", "ê»˜", "í•œí…Œ", "ì—ê²Œì„œ", "í•œí…Œì„œ",
        "ì²˜ëŸ¼", "ê°™ì´", "ë§Œí¼", "ë§Œ", "ë¶€í„°", "ê¹Œì§€", "ì¡°ì°¨", "ë§ˆì €",
        "ë°–ì—", "ë¿", "ë”°ë¼", "ë”°ë¦„", "ë§ˆë‹¤", "ëŒ€ë¡œ", "ì»¤ë…•",
        "ê·¸", "ê·¸ê²ƒ", "ì €", "ì €ê²ƒ", "ì´ê²ƒ", "ê·¸ëŸ°", "ì €ëŸ°", "ì´ëŸ°",
        "ê·¸ë˜ì„œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ëŸ°ë°", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ë¦¬ê³ ", "ê·¸ë¦¬í•˜ì—¬",
        "ë˜", "ë˜í•œ", "ë˜ëŠ”", "ë˜í•œ", "ë˜í•œ", "ë˜í•œ",
        "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ëŸ°ë°", "ê·¸ë ‡ì§€ë§Œ", "ê·¸ëŸ¬ë©´", "ê·¸ë˜ì„œ",
        "ê·¸ë¦¬ê³ ", "ê·¸ë¦¬í•˜ì—¬", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ëŸ°ì¦‰", "ê·¸ëŸ°ì¦‰",
        "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ê·¸ëŸ¬ë‹ˆ", "ê·¸ëŸ¬ë©´", "ê·¸ë˜ì„œ",
    }
    
    # ì˜ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    english_stopwords = {
        "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
        "of", "with", "by", "from", "as", "is", "are", "was", "were", "be",
        "been", "being", "have", "has", "had", "do", "does", "did", "will",
        "would", "should", "could", "may", "might", "must", "can", "this",
        "that", "these", "those", "it", "its", "itself", "they", "them",
        "their", "theirs", "themselves", "what", "which", "who", "whom",
        "whose", "where", "when", "why", "how", "all", "each", "every",
        "both", "few", "more", "most", "other", "some", "such", "no",
        "nor", "not", "only", "own", "same", "so", "than", "too", "very",
    }
    
    # ëª¨ë“  ë¶ˆìš©ì–´ í•©ì¹˜ê¸°
    all_stopwords = stopwords | english_stopwords
    
    # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬
    words = text.split()
    
    # ë¶ˆìš©ì–´ ì œê±° ë° í•œ ê¸€ì ë‹¨ì–´ ì œê±°
    filtered_words = []
    for word in words:
        # ë‹¨ì–´ ì •ë¦¬ (êµ¬ë‘ì  ì œê±°)
        cleaned_word = re.sub(r'[^\w\s$%]', '', word)
        
        # í•œ ê¸€ì ë‹¨ì–´ ì œê±° (ë‹¨, ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìëŠ” ë³´ì¡´)
        if len(cleaned_word) <= 1 and not cleaned_word.isdigit():
            continue
        
        # ë¶ˆìš©ì–´ ì œê±°
        if cleaned_word.lower() in all_stopwords:
            continue
        
        # ê¸ˆìœµ ìˆ«ì íŒ¨í„´ ë³´ì¡´ ($57.0B, 23.5% ë“±)
        if re.match(r'^[\$â‚¬Â£Â¥]?\d+[.,]?\d*[BMKkmb%]?$', cleaned_word):
            filtered_words.append(word)
            continue
        
        # ë‚˜ë¨¸ì§€ ë‹¨ì–´ ì¶”ê°€
        if cleaned_word:
            filtered_words.append(word)
    
    # ë‹¨ì–´ë“¤ì„ ë‹¤ì‹œ ë¬¸ì¥ìœ¼ë¡œ í•©ì¹˜ê¸°
    processed_text = " ".join(filtered_words)
    
    return processed_text


def chunk_text(text: str, max_tokens: int = 1200) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 1200)
        
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    chunks: List[str] = []
    
    # tiktoken ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    try:
        import tiktoken
        encoding = tiktoken.encoding_for_model(API_MODELS["llm"])
        tokens = encoding.encode(text)
        
        for i in range(0, len(tokens), max_tokens):
            token_chunk = tokens[i : i + max_tokens]
            chunks.append(encoding.decode(token_chunk))
    except Exception:
        # tiktokenì´ ì—†ìœ¼ë©´ ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ë¶„í•  (ëŒ€ëµ 4ê¸€ì = 1í† í° ê°€ì •)
        approx_chars = max_tokens * 4
        for i in range(0, len(text), approx_chars):
            chunks.append(text[i : i + approx_chars])
    
    return chunks


# --- [4] PDF íŒŒì‹± í•¨ìˆ˜ë“¤ ---

def extract_text_from_pdf(pdf_path: str) -> str:
    """
    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”: '{pdf_path}'")
    
    # PyMuPDF ì‚¬ìš© ì‹œë„
    try:
        import fitz  # PyMuPDF
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text
    except ImportError:
        raise ImportError("PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì–´ìš”! 'pip install pymupdf'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")


def split_into_sentences(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸
        
    Returns:
        ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    """
    import re
    # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if s.strip()]


def extract_text_from_pdf_with_metadata(pdf_path: str) -> List[Dict[str, Any]]:
    """
    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        [
            {
                "text": "...",
                "page_number": 1,
                "source_file": "report.pdf",
                "sentence_id": "p1_s1",
                "original_sentence": "..."
            },
            ...
        ]
    """
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”: '{pdf_path}'")
    
    try:
        import fitz  # PyMuPDF
        doc = fitz.open(pdf_path)
        chunks_with_metadata = []
        source_file = os.path.basename(pdf_path)
        
        for page_num, page in enumerate(doc, start=1):
            page_text = page.get_text()
            
            if not page_text.strip():
                continue
            
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            sentences = split_into_sentences(page_text)
            
            for sent_id, sentence in enumerate(sentences):
                if len(sentence.strip()) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                    continue
                    
                chunks_with_metadata.append({
                    "text": sentence,
                    "page_number": page_num,
                    "source_file": source_file,
                    "sentence_id": f"p{page_num}_s{sent_id}",
                    "original_sentence": sentence
                })
        
        doc.close()
        return chunks_with_metadata
        
    except ImportError:
        raise ImportError("PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì–´ìš”! 'pip install pymupdf'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")


# --- [5] ê¸ˆìœµ ì—”í‹°í‹° í”„ë¡¬í”„íŠ¸ ---

def get_financial_entity_prompt() -> str:
    """
    ê¸ˆìœµ ì—”í‹°í‹° ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
    
    Returns:
        í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
    """
    return """You are a financial analyst extracting entities from financial documents.

Focus on extracting the following financial entities with HIGH PRIORITY:
- REVENUE (ë§¤ì¶œ, Revenue, Sales, Total Revenue)
- OPERATING_INCOME (ì˜ì—…ì´ìµ, Operating Income, Operating Profit)
- NET_INCOME (ìˆœì´ìµ, Net Income, Profit)
- GROWTH_RATE (ì„±ì¥ë¥ , Growth Rate, YoY Growth, QoQ Growth)
- MARGIN (ë§ˆì§„, Gross Margin, Operating Margin, Net Margin)
- ASSET (ìì‚°, Total Assets, Current Assets)
- LIABILITY (ë¶€ì±„, Total Liabilities, Current Liabilities)
- EQUITY (ìë³¸, Shareholders' Equity, Equity)
- CASH_FLOW (í˜„ê¸ˆíë¦„, Operating Cash Flow, Free Cash Flow)
- EPS (ì£¼ë‹¹ìˆœì´ìµ, Earnings Per Share, EPS)
- PE_RATIO (ì£¼ê°€ìˆ˜ìµë¹„ìœ¨, P/E Ratio, Price-to-Earnings)
- MARKET_CAP (ì‹œê°€ì´ì•¡, Market Capitalization, Market Cap)

Also extract standard entities:
- ORGANIZATION (íšŒì‚¬ëª…, ê¸°ê´€ëª…)
- PERSON (ì¸ë¬¼ëª…, ì„ì›ëª…)
- GEO (ì§€ì—­, êµ­ê°€, ë„ì‹œ)
- DATE (ë‚ ì§œ, ë¶„ê¸°, ì—°ë„)
- TECHNOLOGY (ê¸°ìˆ ëª…, ì œí’ˆëª…)

For each entity, extract:
1. The exact name/value
2. The entity type
3. The context (where it appears in the document)
4. Relationships to other entities (e.g., "NVIDIA's revenue is $57.0B")

Be precise with financial numbers. Extract exact values with units (e.g., "$57.0 billion", "23.5%", "Q3 2026").
"""


def get_strict_grounding_prompt(question: str, sources: List[dict]) -> str:
    """
    ì™¸ë¶€ ì§€ì‹ ì‚¬ìš©ì„ ê¸ˆì§€í•˜ê³  ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ë§Œ ê°•ì œí•˜ëŠ” í”„ë¡¬í”„íŠ¸
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        sources: ì¶œì²˜ ë¦¬ìŠ¤íŠ¸ [{"id": 1, "file": "...", "page_number": 1, "excerpt": "..."}, ...]
    
    Returns:
        Strict grounding system prompt
    """
    if not sources:
        return f"""You are a STRICT document-based analyst.

CRITICAL RULE: The provided documents contain NO information relevant to this question.

QUESTION: {question}

REQUIRED RESPONSE: "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

You MUST respond with exactly this message in Korean."""
    
    sources_text = "\n\n".join([
        f"[{s['id']}] File: {s['file']}, Page: {s.get('page_number', 'N/A')}\n"
        f"Content: {s['excerpt']}\n"
        f"Original: {s.get('original_sentence', s['excerpt'])}"
        for s in sources
    ])
    
    max_citation_num = len(sources)
    
    return f"""You are a professional financial analyst. Create a structured report using **Professional Markdown** format.

ğŸ“‹ CRITICAL RULES:
1. ONLY use information from the provided sources below
2. DO NOT use external knowledge or background information
3. If information is NOT in sources, respond: "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

ğŸ“š AVAILABLE SOURCES:
{sources_text}

â“ QUESTION: {question}

ğŸ“ REQUIRED FORMAT (Professional Markdown):

## í•µì‹¬ ì¸ì‚¬ì´íŠ¸
[2-3 ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‹µë³€ ìš”ì•½, ëª¨ë“  ìˆ˜ì¹˜ëŠ” **êµµê²Œ**, ìš©ì–´ëŠ” `inline code`ë¡œ]

## ì¸ê³¼ê´€ê³„ ë‹¤ì´ì–´ê·¸ë¨
```
A â†’ B â†’ C â†’ ê²°ê³¼
```
[í™”ì‚´í‘œë¡œ ê´€ê³„ë¥¼ ëª…í™•íˆ í‘œí˜„]

## ìƒì„¸ ë¶„ì„

### 1ï¸âƒ£ [ì²« ë²ˆì§¸ ì£¼ìš” í¬ì¸íŠ¸]
- **ìˆ˜ì¹˜ ë°ì´í„°**ëŠ” ë°˜ë“œì‹œ **êµµê²Œ** í‘œì‹œ
- `í•µì‹¬ ìš©ì–´`ëŠ” inline codeë¡œ ê°ì‹¸ê¸°
- êµ¬ì²´ì ì¸ ì‚¬ì‹¤ê³¼ ë°ì´í„° ì¤‘ì‹¬

### 2ï¸âƒ£ [ë‘ ë²ˆì§¸ ì£¼ìš” í¬ì¸íŠ¸]
- ê´€ë ¨ ì„¸ë¶€ì‚¬í•­ê³¼ ë§¥ë½
- ë¹„êµ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í…Œì´ë¸” ì‚¬ìš©

### 3ï¸âƒ£ [ì„¸ ë²ˆì§¸ ì£¼ìš” í¬ì¸íŠ¸] (í•„ìš”ì‹œ)
- ì¶”ê°€ ë¶„ì„ ë‚´ìš©

## ë°ì´í„° ìš”ì•½ 
| í•­ëª© | ìˆ˜ì¹˜ |
|------|------|
| ì˜ˆì‹œ | **$100M** |

## ì—ì´ì „íŠ¸ì˜ í•œ ì¤„ í‰
**[ë¶„ì„ ê²°ê³¼ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½]**

FORMATTING RULES:
âœ… ëª¨ë“  ìˆ˜ì¹˜ëŠ” **êµµê²Œ** (ì˜ˆ: **$57B**, **22%**)
âœ… í•µì‹¬ ìš©ì–´ëŠ” `inline code` (ì˜ˆ: `TSMC`, `HBM`, `AI GPU`)
âœ… ì¸ê³¼ê´€ê³„ëŠ” A â†’ B â†’ C ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ
âœ… í…Œì´ë¸”ì€ Markdown Table í˜•ì‹
âœ… ë§ˆì§€ë§‰ì— **ì—ì´ì „íŠ¸ì˜ í•œ ì¤„ í‰** í•„ìˆ˜
âŒ ì¸ìš© ë²ˆí˜¸ [1], [2] ì‚¬ìš© ê¸ˆì§€
âŒ HTML íƒœê·¸ ì‚¬ìš© ê¸ˆì§€
âŒ ë³„ë„ ì¶œì²˜ ì„¹ì…˜ ì‘ì„± ê¸ˆì§€

Begin your Professional Markdown response:"""


def get_executive_report_prompt(question: str, sources: List[dict]) -> str:
    """
    ì„ì›ê¸‰ ë³´ê³ ì„œ í˜•ì‹ì˜ System Prompt ìƒì„±
    Perplexity ìŠ¤íƒ€ì¼ë¡œ Citationì´ í¬í•¨ëœ ì „ë¬¸ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ë„ë¡ ìœ ë„
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        sources: ì¶œì²˜ ë¦¬ìŠ¤íŠ¸ [{"id": 1, "file": "...", "excerpt": "..."}, ...]
    
    Returns:
        System prompt string
    """
    # ì¶œì²˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬ë§·íŒ…
    sources_text = "\n".join([
        f"[{s['id']}] {s['file']} (Chunk {s.get('chunk_id', 'N/A')}): \"{s['excerpt'][:150]}...\""
        for s in sources
    ])
    
    max_citation_num = len(sources) if sources else 0
    
    prompt = f"""You are an elite executive analyst preparing a professional report for C-level executives.

QUESTION: {question}

AVAILABLE SOURCES (Citation numbers: [1] to [{max_citation_num}]):
{sources_text if sources else "[No specific sources available - use general knowledge]"}

REPORT STRUCTURE (MANDATORY):

## EXECUTIVE SUMMARY
Provide a concise 2-3 sentence overview of the key findings. This should be immediately actionable for decision-makers.

## KEY FINDINGS
Present 3-5 bullet points highlighting the most critical insights. **EVERY finding MUST include citation [1], [2], etc.**
- Finding 1 with supporting data [1]
- Finding 2 with evidence [2]
- Continue with [3], [4] as needed

## DETAILED ANALYSIS  
Provide in-depth analysis with clear sections:
- Break down complex information into digestible parts
- **Support EVERY factual claim with citations [1], [2], [3]**
- Example: "TSMC announced 690ì–µ ë‹¬ëŸ¬ revenue [1]."
- Use quantitative data where available
- Explain implications and context

## CONCLUSION & RECOMMENDATIONS
Summarize the analysis and provide actionable recommendations:
- Key takeaways
- Strategic implications  
- Recommended next steps

CITATION RULES (CRITICAL):
1. After EVERY factual statement, immediately add [1], [2], [3] etc. corresponding to the source
2. Available citation range: [1] through [{max_citation_num}] ONLY
3. Citation placement: "Statement here [1]." NOT "Statement here. [1]"
4. Multiple citations allowed: [1][2] or [1, 2]
5. ONLY use source numbers provided above - do NOT invent citations
6. If no sources available, do NOT use citations - state it's based on general knowledge
7. **Be generous with citations - better to over-cite than under-cite**
8. Cite specific data points, quotes, and factual claims

FORMATTING:
- Use clear markdown headers (##)
- Use bold (**text**) for emphasis
- Use bullet points for lists
- Keep paragraphs concise (3-4 sentences max)
- Professional, data-driven tone
- PLAIN TEXT ONLY - absolutely NO HTML tags or markup

IMPORTANT: 
- Do NOT add a separate References section at the end (it will be added automatically)
- Do NOT generate HTML code (<a>, <div>, etc.) - text only
- Focus on insights, not just data regurgitation
- Be precise with numbers and dates
- If data is insufficient, acknowledge limitations
- Every paragraph should have at least one citation if it contains factual information

Begin your report now (PLAIN TEXT, NO HTML, CITE FREQUENTLY):"""
    
    return prompt


def get_web_search_report_prompt(question: str, search_results: List[dict]) -> str:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” System Prompt
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        search_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼ [{"title": "...", "snippet": "...", "url": "..."}, ...]
    
    Returns:
        System prompt string
    """
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì†ŒìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    sources_text = "\n".join([
        f"[{idx}] {result['title']}\n   Source: {result['url']}\n   Content: \"{result['snippet'][:200]}...\""
        for idx, result in enumerate(search_results, 1)
    ])
    
    prompt = f"""You are an elite research analyst synthesizing web search results into an executive report.

QUESTION: {question}

WEB SEARCH RESULTS:
{sources_text}

REPORT STRUCTURE (MANDATORY):

## EXECUTIVE SUMMARY
Synthesize the web findings into 2-3 actionable sentences.

## KEY FINDINGS
Present 3-5 bullet points from the search results. Cite sources [1], [2], etc.

## DETAILED ANALYSIS
Synthesize information from multiple sources:
- Compare and contrast different perspectives
- Identify trends and patterns
- Cite sources for every claim [1][2][3]

## CONCLUSION & RECOMMENDATIONS
Based on the web research, provide strategic insights.

CITATION RULES:
- Cite web sources as [1], [2], [3] matching the search results above
- Every factual claim needs a citation
- Synthesize multiple sources when appropriate [1][2]

Begin your report now:"""
    
    return prompt

