"""
Connection Check - Verify Local Security Model is Running
ë¡œì»¬ ë³´ì•ˆ ëª¨ë¸ ì—°ê²° í™•ì¸ ëª¨ë“ˆ
"""

import requests
import sys
from typing import Tuple, Dict, List


class ConnectionChecker:
    """
    ë¡œì»¬ ë³´ì•ˆ ëª¨ë¸ ì—°ê²° ì²´í¬
    
    Security Policy:
    - Ollama ë¡œì»¬ ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì´ì§€ ì•Šìœ¼ë©´ ì‹œìŠ¤í…œ ì¢…ë£Œ
    - ë¯¼ê° ë°ì´í„° ì²˜ë¦¬ ì „ ë°˜ë“œì‹œ ë¡œì»¬ ëª¨ë¸ í™•ì¸
    - í´ë¼ìš°ë“œ APIë¡œ í´ë°± ì ˆëŒ€ ë¶ˆê°€
    """
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        self.required_model = "qwen2.5-coder"
    
    def check_ollama_connection(self) -> Tuple[bool, str, List[str]]:
        """
        Ollama ì„œë²„ ì—°ê²° ë° ëª¨ë¸ í™•ì¸
        
        Returns:
            (ì—°ê²° ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€, ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡)
        """
        try:
            response = requests.get(
                f"{self.ollama_url}/api/tags",
                timeout=3
            )
            
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                model_names = [m.get("name", "") for m in models]
                
                return True, "Ollama is running", model_names
            else:
                return False, f"Ollama returned status {response.status_code}", []
                
        except requests.exceptions.ConnectionError:
            return False, f"Cannot connect to Ollama at {self.ollama_url}", []
        except requests.exceptions.Timeout:
            return False, "Ollama connection timeout", []
        except Exception as e:
            return False, f"Ollama check failed: {str(e)}", []
    
    def verify_required_model(self) -> Tuple[bool, str]:
        """
        í•„ìˆ˜ ëª¨ë¸(qwen2.5-coder) ì¡´ì¬ í™•ì¸
        
        Returns:
            (ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        is_connected, message, models = self.check_ollama_connection()
        
        if not is_connected:
            return False, message
        
        # Check if required model exists
        model_found = any(self.required_model in model for model in models)
        
        if model_found:
            return True, f"Required model '{self.required_model}' is available"
        else:
            return False, f"Required model '{self.required_model}' not found. Available models: {', '.join(models)}"
    
    def enforce_local_model_or_exit(self):
        """
        ë¡œì»¬ ëª¨ë¸ ê°•ì œ í™•ì¸ - ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ì¢…ë£Œ
        
        Security Critical:
        - ì´ í•¨ìˆ˜ëŠ” ë¯¼ê° ë°ì´í„° ì²˜ë¦¬ ì „ì— ë°˜ë“œì‹œ í˜¸ì¶œë˜ì–´ì•¼ í•¨
        - ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        """
        print("\n" + "=" * 70)
        print("ğŸ”’ SECURITY CHECK: Verifying Local Security Model")
        print("=" * 70)
        
        is_available, message = self.verify_required_model()
        
        if is_available:
            print(f"âœ… {message}")
            print("âœ… Local security model is ready for sensitive data processing")
            print("=" * 70 + "\n")
            return True
        else:
            print(f"âŒ {message}")
            print("\n" + "!" * 70)
            print("ğŸš¨ SECURITY VIOLATION DETECTED")
            print("!" * 70)
            print("\në¡œì»¬ ë³´ì•ˆ ëª¨ë¸ì´ êµ¬ë™ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ë³´ì•ˆì„ ìœ„í•´ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            print("\ní•„ìˆ˜ ì¡°ì¹˜:")
            print("1. Ollama ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”: ollama serve")
            print(f"2. í•„ìˆ˜ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: ollama pull {self.required_model}")
            print("3. ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: ollama list")
            print("\n" + "!" * 70)
            print("SYSTEM SHUTDOWN FOR SECURITY")
            print("!" * 70 + "\n")
            
            # Force exit
            sys.exit(1)


def check_local_model_before_processing():
    """
    ë¯¼ê° ë°ì´í„° ì²˜ë¦¬ ì „ ë¡œì»¬ ëª¨ë¸ í™•ì¸
    
    Usage:
        from engine.connection_check import check_local_model_before_processing
        check_local_model_before_processing()  # Will exit if local model not available
    """
    checker = ConnectionChecker()
    checker.enforce_local_model_or_exit()


def get_local_model_status() -> Dict[str, any]:
    """
    ë¡œì»¬ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ (ì •ë³´ í™•ì¸ìš©)
    
    Returns:
        ìƒíƒœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    checker = ConnectionChecker()
    is_connected, message, models = checker.check_ollama_connection()
    is_model_ready, model_message = checker.verify_required_model()
    
    return {
        "ollama_running": is_connected,
        "connection_message": message,
        "available_models": models,
        "required_model_ready": is_model_ready,
        "model_message": model_message
    }


if __name__ == "__main__":
    """Test connection checker"""
    print("Testing Local Security Model Connection...")
    
    checker = ConnectionChecker()
    
    # Test 1: Check connection
    print("\n1. Checking Ollama connection...")
    is_connected, message, models = checker.check_ollama_connection()
    print(f"   Connected: {is_connected}")
    print(f"   Message: {message}")
    print(f"   Models: {models}")
    
    # Test 2: Verify required model
    print("\n2. Verifying required model...")
    is_ready, model_msg = checker.verify_required_model()
    print(f"   Ready: {is_ready}")
    print(f"   Message: {model_msg}")
    
    # Test 3: Enforce (will exit if not available)
    print("\n3. Enforcing local model requirement...")
    checker.enforce_local_model_or_exit()
    
    print("\nâœ… All checks passed!")
