# app.pyëŠ” "FastAPI ì„œë²„"ë¥¼ ë§Œë“œëŠ” íŒŒì¼ì´ì—ìš”!
# ë§ˆì¹˜ "ì›¹ ì„œë²„ë¥¼ ë§Œë“œëŠ” ë„êµ¬ ìƒì" ê°™ì€ ê±°ì˜ˆìš”!

from fastapi import FastAPI, HTTPException, Query, UploadFile, File
from fastapi.responses import FileResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
import uvicorn
import os
import sys

# src ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€í•´ìš”!
# ì´ë ‡ê²Œ í•˜ë©´ 'from engine import ...' ê°™ì€ importê°€ ì‘ë™í•´ìš”!
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# engine ëª¨ë“ˆì—ì„œ HybridGraphRAGEngineì„ ê°€ì ¸ì™€ìš”!
from engine import HybridGraphRAGEngine
from config import (
    print_config,
    validate_config,
    validate_privacy_mode,
    ROUTER_MODEL,
    ROUTER_TEMPERATURE,
    WEB_SEARCH_MAX_RESULTS,
    OPENAI_API_KEY,
    OPENAI_BASE_URL,
    PRIVACY_MODE,
    NEO4J_URI,
    NEO4J_USERNAME,
    NEO4J_PASSWORD
)
from openai import AsyncOpenAI
from utils import get_executive_report_prompt, get_web_search_report_prompt
try:
    from utils.error_logger import droneLogError
except Exception:
    def droneLogError(message: str, error: Exception | None = None) -> None:
        return

# --- [1] ì „ì—­ ë³€ìˆ˜ ---
# engineì€ "GraphRAG ì—”ì§„"ì´ì—ìš”!
# Noneì€ "ì•„ì§ ì•„ë¬´ê²ƒë„ ì—†ë‹¤"ëŠ” ëœ»ì´ì—ìš”!
engine: HybridGraphRAGEngine = None
mcp_manager = None
neo4j_db = None
agentic_workflow = None

# --- [2] ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
# @asynccontextmanagerëŠ” "ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"ë¥¼ ë§Œë“œëŠ” ê±°ì˜ˆìš”!
# ë§ˆì¹˜ "ì„œë²„ê°€ ì‹œì‘ë  ë•Œì™€ ëë‚  ë•Œ ë­”ê°€ë¥¼ í•˜ëŠ”" ê²ƒì²˜ëŸ¼!
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ì´ì—ìš”!
    global engine, mcp_manager, neo4j_db, agentic_workflow
    
    # ì„¤ì • ì •ë³´ë¥¼ ì¶œë ¥í•´ìš”!
    print_config()
    
    # validate_config()ëŠ” "ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ëŠ”" í•¨ìˆ˜ì˜ˆìš”!
    validate_config()
    
    # Privacy Mode ê²€ì¦ ë° ì§„ë‹¨
    print("\nğŸ” Privacy Mode ì§„ë‹¨ ì‹œì‘...")
    privacy_validation = validate_privacy_mode()
    
    if not privacy_validation["valid"]:
        print("âš ï¸  Privacy Mode ê²€ì¦ ì‹¤íŒ¨:")
        for error in privacy_validation["errors"]:
            print(f"   âŒ {error}")
        print("\nğŸ’¡ ì‹œìŠ¤í…œì´ degraded modeë¡œ ì‹œì‘ë©ë‹ˆë‹¤.")
        print("   ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
    
    if privacy_validation["warnings"]:
        print("âš ï¸  ê²½ê³ :")
        for warning in privacy_validation["warnings"]:
            print(f"   âš ï¸  {warning}")
    
    # Neo4j ping í…ŒìŠ¤íŠ¸
    if NEO4J_URI and NEO4J_PASSWORD:
        print("\nğŸ” Neo4j ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        try:
            from db.neo4j_db import Neo4jDatabase
            neo4j_db = Neo4jDatabase(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)
            # Test connection
            neo4j_db.execute_query("RETURN 1")
            print(f"âœ… Neo4j ì—°ê²° ì„±ê³µ: {NEO4J_URI}")
            neo4j_db.close()
            
        except Exception as e:
            print(f"âŒ Neo4j ì§„ë‹¨ ì‹¤íŒ¨: {e}")
            print("   ì¿¼ë¦¬ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.\n")
    
    # HybridGraphRAGEngineì„ ì´ˆê¸°í™”í•˜ëŠ” ê±°ì˜ˆìš”!
    # Privacy Mode ì „ìš©ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    print("ğŸš€ PrivacyGraphRAGEngine ì´ˆê¸°í™” ì¤‘...")
    try:
        engine = HybridGraphRAGEngine()
        print("âœ… PrivacyGraphRAGEngine ì¤€ë¹„ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ Engine ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("   degraded modeë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
        engine = None
    
    # MCP Manager ì´ˆê¸°í™” (ì˜µì…˜)
    try:
        from mcp.manager import MCPManager
        print("ğŸ”§ MCP Manager ì´ˆê¸°í™” ì¤‘...")
        mcp_manager = MCPManager()
        print("âœ… MCP Manager ì¤€ë¹„ ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸ MCP Manager ì´ˆê¸°í™” ì‹¤íŒ¨ (ì˜µì…˜): {e}")
        mcp_manager = None
    
    # Neo4j DB ì´ˆê¸°í™” (ì˜µì…˜)
    try:
        from db.neo4j_db import Neo4jDatabase
        print("ğŸ”§ Neo4j Database ì´ˆê¸°í™” ì¤‘...")
        neo4j_db = Neo4jDatabase()
        print("âœ… Neo4j Database ì¤€ë¹„ ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸ Neo4j Database ì´ˆê¸°í™” ì‹¤íŒ¨ (ì˜µì…˜): {e}")
        neo4j_db = None
    
    # Agentic Workflow ì´ˆê¸°í™”
    # #region agent log
    import json
    with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
        f.write(json.dumps({"location":"app.py:70","message":"Agentic Workflow init start","data":{"engine_ready":engine is not None,"mcp_ready":mcp_manager is not None,"neo4j_ready":neo4j_db is not None},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H2,H4"})+'\n')
    # #endregion
    try:
        # #region agent log
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:72","message":"Before import AgenticWorkflow","data":{},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H5"})+'\n')
        # #endregion
        from agents.langgraph_workflow import AgenticWorkflow
        # #region agent log
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:73","message":"After import AgenticWorkflow","data":{"class_type":str(type(AgenticWorkflow))},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H5"})+'\n')
        # #endregion
        print("ğŸ”§ Agentic Workflow ì´ˆê¸°í™” ì¤‘...")
        # #region agent log
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:74","message":"Before AgenticWorkflow instantiation","data":{},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H2,H3"})+'\n')
        # #endregion
        agentic_workflow = AgenticWorkflow(
            engine=engine,
            mcp_manager=mcp_manager,
            neo4j_db=neo4j_db
        )
        # #region agent log
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:79","message":"After AgenticWorkflow instantiation","data":{"workflow_ready":agentic_workflow is not None},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H3"})+'\n')
        # #endregion
        print("âœ… Agentic Workflow ì¤€ë¹„ ì™„ë£Œ!")
    except Exception as e:
        # #region agent log
        import traceback
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:80","message":"Agentic Workflow exception caught","data":{"error_type":type(e).__name__,"error_msg":str(e),"traceback":traceback.format_exc()},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H2,H3,H4,H5"})+'\n')
        # #endregion
        print(f"âš ï¸ Agentic Workflow ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        agentic_workflow = None
    
    # yieldëŠ” "ì—¬ê¸°ì„œ ì ì‹œ ë©ˆì¶°ì„œ ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê³ , ë‚˜ì¤‘ì— ë‹¤ì‹œ ëŒì•„ì™€"ë¼ëŠ” ëœ»ì´ì—ìš”!
    yield
    
    # ì„œë²„ê°€ ì¢…ë£Œë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ì´ì—ìš”!
    if mcp_manager:
        print("ğŸ”’ MCP Manager ì¢…ë£Œ ì¤‘...")
        await mcp_manager.shutdown()
        print("âœ… MCP Manager ì¢…ë£Œ ì™„ë£Œ!")
    
    if neo4j_db:
        print("ğŸ”’ Neo4j Database ì¢…ë£Œ ì¤‘...")
        neo4j_db.close()
        print("âœ… Neo4j Database ì¢…ë£Œ ì™„ë£Œ!")

# --- [3] FastAPI ì•± ì´ˆê¸°í™” ---
# FastAPI()ëŠ” "ì›¹ ì„œë²„ ì•±ì„ ë§Œë“¤ì–´ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
# lifespanì€ "ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"ì˜ˆìš”!
app = FastAPI(
    title="VIK AI: Hybrid GraphRAG API",
    description="ê¸ˆìœµ ë¶„ì„ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ GraphRAG APIì˜ˆìš”! ì¸ë±ì‹±ì€ OpenAI API, ì§ˆë¬¸ì€ API/LOCAL ì„ íƒ ê°€ëŠ¥í•´ìš”!",
    version="2.0.0",
    lifespan=lifespan  # lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ì—°ê²°í•´ìš”!
)

# --- [4] Pydantic ëª¨ë¸ ---
# Pydantic ëª¨ë¸ì€ "ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒ"ì´ì—ìš”!
# ë§ˆì¹˜ "ì´ëŸ° ëª¨ì–‘ì˜ ë°ì´í„°ë¥¼ ë°›ì„ê²Œìš”!"ë¼ê³  ë¯¸ë¦¬ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”!

# QueryRequestëŠ” "ì§ˆë¬¸ ìš”ì²­"ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì´ì—ìš”!
class QueryRequest(BaseModel):
    # questionì€ "ì§ˆë¬¸ ë‚´ìš©"ì´ì—ìš”!
    question: str
    # modeëŠ” "ì–´ë–¤ ëª¨ë“œë¥¼ ì‚¬ìš©í• ì§€" ì •í•˜ëŠ” ê±°ì˜ˆìš”. "api" ë˜ëŠ” "local"!
    # ê¸°ë³¸ê°’ì€ "local"ì´ì—ìš”!
    mode: str = "local"
    # temperatureëŠ” "ì‘ë‹µì˜ ì°½ì˜ì„±"ì„ ì¡°ì ˆí•´ìš”! (0.0 = ì •í™•, 2.0 = ì°½ì˜ì )
    temperature: float = 0.2
    # top_këŠ” "ê²€ìƒ‰í•  ì²­í¬ ê°œìˆ˜"ë¥¼ ì •í•´ìš”!
    top_k: int = 30
    # search_typeì€ "local" (íŠ¹ì • ê²€ìƒ‰) ë˜ëŠ” "global" (ì „ì²´ ìš”ì•½)
    search_type: str = "local"
    # enable_web_searchëŠ” "ì›¹ ê²€ìƒ‰ì„ í™œì„±í™”í• ì§€" ì •í•´ìš”! (ê¸°ë³¸ê°’: False)
    enable_web_search: bool = False
    
    # Pydantic v2 ìŠ¤íƒ€ì¼ë¡œ ì˜ˆì‹œ ì„¤ì •
    model_config = {
        "json_schema_extra": {
            "example": {
                "question": "What is NVIDIA revenue?",
                "mode": "local",
                "search_type": "local",
                "enable_web_search": False
            }
        }
    }

# InsertRequestëŠ” "í…ìŠ¤íŠ¸ ì¶”ê°€ ìš”ì²­"ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì´ì—ìš”!
class InsertRequest(BaseModel):
    # textëŠ” "ì¶”ê°€í•  í…ìŠ¤íŠ¸"ì˜ˆìš”!
    text: str
    
    # Pydantic v2 ìŠ¤íƒ€ì¼ë¡œ ì„¤ì • (deprecation warning í•´ê²°)
    model_config = {
        "json_schema_extra": {
            "example": {
                "text": "NVIDIA reported record revenue of $57.0 billion in Q3 2026."
            }
        }
    }

# --- [5] Router í•¨ìˆ˜ë“¤ (Decision Layer) ---
# ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì›¹ ê²€ìƒ‰ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë“¤ì´ì—ìš”!

async def classify_query(question: str) -> str:
    """
    GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” Router í•¨ìˆ˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        "GRAPH_RAG" ë˜ëŠ” "WEB_SEARCH"
    """
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        # ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """You are a query classifier for a financial AI system.

Your task is to classify user questions into two categories:

1. GRAPH_RAG: Questions about uploaded PDF documents, company information, people, financials from internal reports
   Examples:
   - "What is NVIDIA's Q3 revenue?"
   - "Who is Jensen Huang?" (person information from documents)
   - "How old is the CEO?" (biographical information)
   - "Summarize the uploaded report"
   - "What are the key findings in the document?"

2. WEB_SEARCH: Questions EXPLICITLY requiring TODAY's/LATEST/CURRENT real-time market data or breaking news
   Examples:
   - "What is today's stock price?"
   - "Latest news TODAY about Tesla"
   - "Current inflation rate RIGHT NOW"
   - "What happened in the market TODAY?"

IMPORTANT: Default to GRAPH_RAG unless the question EXPLICITLY asks for TODAY/LATEST/CURRENT/NOW information.

Respond with ONLY ONE WORD: Either "GRAPH_RAG" or "WEB_SEARCH" - nothing else."""

        # GPT-4o-mini í˜¸ì¶œ
        response = await client.chat.completions.create(
            model=ROUTER_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Classify this question: {question}"}
            ],
            temperature=ROUTER_TEMPERATURE,
            max_tokens=10
        )
        
        # ì‘ë‹µ ì¶”ì¶œ ë° ì •ê·œí™”
        classification = response.choices[0].message.content.strip().upper()
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if "GRAPH_RAG" in classification:
            return "GRAPH_RAG"
        elif "WEB_SEARCH" in classification or "WEB" in classification:
            return "WEB_SEARCH"
        else:
            # ê¸°ë³¸ê°’: GRAPH_RAG (ë‚´ë¶€ ë¬¸ì„œ ìš°ì„ )
            print(f"âš ï¸ ë¶„ë¥˜ ê²°ê³¼ê°€ ëª…í™•í•˜ì§€ ì•Šì•„ìš”: {classification}, ê¸°ë³¸ê°’ GRAPH_RAG ì‚¬ìš©")
            return "GRAPH_RAG"
    
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’: GRAPH_RAG
        return "GRAPH_RAG"


async def handle_web_search(question: str) -> str:
    """
    ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    Note: Legacy function - web search is now handled by Multi-Agent system with MCP Tavily
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€
    """
    # Web search is now handled by Multi-Agent system with MCP Tavily
    return "ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì€ Multi-Agent ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. Advanced Settingsì—ì„œ 'Multi-Agent Analysis Mode'ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”."


# --- [6] ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/")ëŠ” "ë£¨íŠ¸ ê²½ë¡œ(/)ì— GET ìš”ì²­ì´ ì˜¤ë©´" ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ì˜ˆìš”!
# ë§ˆì¹˜ "í™ˆí˜ì´ì§€ì— ì ‘ì†í•˜ë©´" ì‹¤í–‰ë˜ëŠ” ê±°ì˜ˆìš”!
@app.get("/")
async def root():
    # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
    return {
        "message": "VIK AI Hybrid GraphRAG APIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•´ìš”!",
        "description": "ì¸ë±ì‹±ì€ OpenAI API(gpt-5-mini)ë¥¼ ì‚¬ìš©í•˜ê³ , ì§ˆë¬¸ì€ API/LOCAL ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!",
        "endpoints": {
            "/insert": "í…ìŠ¤íŠ¸ ì¸ë±ì‹±í•˜ê¸° (POST) - OpenAI API ì‚¬ìš©",
            "/query": "ì§ˆë¬¸í•˜ê¸° (POST) - mode íŒŒë¼ë¯¸í„°ë¡œ 'api' ë˜ëŠ” 'local' ì„ íƒ",
            "/health": "ì„œë²„ ìƒíƒœ í™•ì¸ (GET)",
            "/graph_stats": "ê·¸ë˜í”„ í˜„í™© í™•ì¸ (GET)",
            "/visualize": "ê·¸ë˜í”„ ì‹œê°í™” HTML ìƒì„± (GET)",
            "/docs": "API ë¬¸ì„œ ë³´ê¸° (GET)"
        },
        "usage": {
            "insert": {
                "method": "POST",
                "url": "/insert",
                "body": {"text": "ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸"}
            },
            "query": {
                "method": "POST",
                "url": "/query",
                "body": {
                    "question": "ì§ˆë¬¸ ë‚´ìš©",
                    "mode": "api ë˜ëŠ” local (ê¸°ë³¸ê°’: local)"
                }
            }
        }
    }

# --- [7] ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/health")ëŠ” "ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/health")
async def health():
    # ì„œë²„ê°€ ì˜ ì‘ë™í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”!
    return {
        "status": "healthy",
        "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì´ì—ìš”!",
        "engine_ready": engine is not None
    }

# --- [8] ê·¸ë˜í”„ í†µê³„ ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/graph_stats")ëŠ” "ê·¸ë˜í”„ í†µê³„ë¥¼ ë³´ì—¬ì£¼ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/graph_stats")
async def graph_stats():
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        return {"nodes": 0, "edges": 0, "message": "ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!"}
    
    # engine.get_graph_stats()ëŠ” ê·¸ë˜í”„ í†µê³„ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê±°ì˜ˆìš”!
    return engine.get_graph_stats()

# --- [9] ê·¸ë˜í”„ ì´ˆê¸°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# @app.post("/reset")ëŠ” "ê·¸ë˜í”„ë¥¼ ì´ˆê¸°í™”í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.post("/reset",
          summary="ê·¸ë˜í”„ ì´ˆê¸°í™”",
          description="ê¸°ì¡´ ê·¸ë˜í”„ ìŠ¤í† ë¦¬ì§€ë¥¼ ë°±ì—…í•˜ê³  ì‚­ì œí•œ í›„ ìƒˆë¡œìš´ ê·¸ë˜í”„ë¡œ ì‹œì‘í•´ìš”!")
async def reset_graph():
    global engine
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    try:
        import shutil
        from datetime import datetime
        
        # ë°±ì—… í´ë” ì´ë¦„ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        backup_dir = f"{engine.working_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ê¸°ì¡´ ê·¸ë˜í”„ ìŠ¤í† ë¦¬ì§€ê°€ ìˆìœ¼ë©´ ë°±ì—…
        if os.path.exists(engine.working_dir):
            shutil.move(engine.working_dir, backup_dir)
            print(f"âœ… ê¸°ì¡´ ê·¸ë˜í”„ ë°±ì—… ì™„ë£Œ: {backup_dir}")
        
        # ì—”ì§„ ì¬ì´ˆê¸°í™”
        engine = HybridGraphRAGEngine()
        
        return {
            "message": "ê·¸ë˜í”„ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆì–´ìš”!",
            "status": "success",
            "backup_dir": backup_dir if os.path.exists(backup_dir) else None
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê·¸ë˜í”„ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [10] ê·¸ë˜í”„ ì‹œê°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# @app.get("/visualize")ëŠ” "ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ëŠ” HTML íŒŒì¼ì„ ìƒì„±í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
@app.get("/visualize",
         summary="ê·¸ë˜í”„ ì‹œê°í™”",
         description="GraphRAG ê·¸ë˜í”„ë¥¼ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ ì‹œê°í™”í•œ HTML íŒŒì¼ì„ ìƒì„±í•´ìš”!")
async def visualize():
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    try:
        # visualize.pyì—ì„œ visualize_graph í•¨ìˆ˜ë¥¼ ê°€ì ¸ì™€ìš”!
        from visualize import visualize_graph
        
        # ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•´ì„œ HTML íŒŒì¼ì„ ìƒì„±í•´ìš”!
        output_path = visualize_graph(working_dir=engine.working_dir, output_file="graph_visualization.html")
        
        if output_path and os.path.exists(output_path):
            # FileResponseëŠ” "íŒŒì¼ì„ ë°˜í™˜í•˜ëŠ”" ê±°ì˜ˆìš”!
            # ë§ˆì¹˜ "ì´ HTML íŒŒì¼ì„ ë¸Œë¼ìš°ì €ë¡œ ë³´ì—¬ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
            return FileResponse(
                output_path,
                media_type="text/html",
                filename="graph_visualization.html"
            )
        else:
            raise HTTPException(status_code=500, detail="ê·¸ë˜í”„ ì‹œê°í™” íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ìš”!")
            
    except ImportError:
        raise HTTPException(status_code=500, detail="pyvis íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì–´ìš”! 'pip install pyvis'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”!")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [11] í…ìŠ¤íŠ¸ ì¸ë±ì‹± ì—”ë“œí¬ì¸íŠ¸ ---
# @app.post("/insert")ëŠ” "í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ì‹±í•˜ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
# ì¸ë±ì‹±ì€ í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”! (ì •í™•í•œ ê¸ˆìœµ ìˆ˜ì¹˜ ì¶”ì¶œì„ ìœ„í•´)
@app.post("/insert", 
          summary="í…ìŠ¤íŠ¸ ì¸ë±ì‹±",
          description="í…ìŠ¤íŠ¸ë¥¼ GraphRAGì— ì¸ë±ì‹±í•´ìš”. í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”!",
          response_description="ì¸ë±ì‹± ê²°ê³¼")
async def insert(request: InsertRequest):
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        # HTTPExceptionì€ "ì—ëŸ¬ë¥¼ ë˜ì§€ëŠ”" ê±°ì˜ˆìš”!
        # 503ì€ "ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    # text í•„ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if not request.text or not request.text.strip():
        raise HTTPException(
            status_code=422, 
            detail="'text' í•„ë“œëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ì–´ìš”! í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )
    
    try:
        # tryëŠ” "ì‹œë„í•´ë´"ë¼ëŠ” ëœ»ì´ì—ìš”!
        # engine.ainsert()ëŠ” ë¹„ë™ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë˜í”„ì— ë„£ëŠ” ê±°ì˜ˆìš”!
        # ì¸ë±ì‹±ì€ í•­ìƒ OpenAI APIë¥¼ ì‚¬ìš©í•´ìš”!
        await engine.ainsert(request.text)
        
        # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
        return {
            "message": "í…ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì¸ë±ì‹±ë˜ì—ˆì–´ìš”! (OpenAI API ì‚¬ìš©)",
            "status": "success",
            "mode": "openai_api"
        }
    except Exception as e:
        # exceptëŠ” "ë§Œì•½ ì—ëŸ¬ê°€ ìƒê¸°ë©´"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        # Exceptionì€ "ëª¨ë“  ì¢…ë¥˜ì˜ ì—ëŸ¬"ì˜ˆìš”!
        # eëŠ” ì—ëŸ¬ ë‚´ìš©ì´ì—ìš”!
        # HTTPExceptionìœ¼ë¡œ ì—ëŸ¬ë¥¼ ë°˜í™˜í•´ìš”!
        import traceback
        error_detail = f"ì¸ë±ì‹± ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}\n\nìƒì„¸ ì •ë³´:\n{traceback.format_exc()}"
        print(f"âŒ ì¸ë±ì‹± ì—ëŸ¬:\n{error_detail}")
        raise HTTPException(status_code=500, detail=f"ì¸ë±ì‹± ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")

# --- [12] ì§ˆë¬¸-ë‹µë³€ ì—”ë“œí¬ì¸íŠ¸ (Decision Layer í†µí•©) ---
# --- [7] Agentic Query Endpoint ---
@app.post("/agentic-query",
          summary="Agentic Workflow ì§ˆë¬¸-ë‹µë³€",
          description="LangGraph ê¸°ë°˜ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¡œ ì§ˆë¬¸ ì²˜ë¦¬ (Planner â†’ Collector â†’ Analyst â†’ Writer)")
async def agentic_query(request: QueryRequest):
    """
    Agentic Workflowë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸-ë‹µë³€
    
    ì›Œí¬í”Œë¡œìš°:
    1. Planner: ì§ˆë¬¸ì„ ì„œë¸ŒíƒœìŠ¤í¬ë¡œ ë¶„í•´
    2. Collector: ê° ì„œë¸ŒíƒœìŠ¤í¬ë³„ ì •ë³´ ìˆ˜ì§‘ + Neo4j ì €ì¥
    3. Analyst: ë°ì´í„° ê²€ì¦ + ì¶©ë¶„ì„± íŒë‹¨ (ë¶€ì¡± ì‹œ Collectorë¡œ íšŒê·€)
    4. Writer: ìµœì¢… ë¦¬í¬íŠ¸ ì‘ì„± + ì¶”ë¡  ê²½ë¡œ í¬í•¨
    """
    if agentic_workflow is None:
        raise HTTPException(
            status_code=503,
            detail="Agentic Workflowê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )
    
    if not request.question or not request.question.strip():
        raise HTTPException(
            status_code=422,
            detail="'question' í•„ë“œëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
    
    try:
        print(f"\n{'='*60}")
        print(f"[Agentic Workflow] ì§ˆë¬¸: {request.question}")
        print(f"{'='*60}\n")
        
        # Agentic Workflow ì‹¤í–‰ (ìµœëŒ€ 3íšŒ Feedback Loop)
        result = await agentic_workflow.run(
            question=request.question,
            max_iterations=3
        )
        
        print(f"\n{'='*60}")
        print(f"[Agentic Workflow] ì™„ë£Œ!")
        print(f"- ì„œë¸ŒíƒœìŠ¤í¬: {len(result.get('subtasks', []))}ê°œ")
        print(f"- ë°˜ë³µ íšŸìˆ˜: {result.get('iteration_count', 0)}íšŒ")
        print(f"- ì‹ ë¢°ë„: {result.get('confidence', 0):.0%}")
        print(f"- ì¶”ì²œ: {result.get('recommendation', 'N/A')}")
        print(f"{'='*60}\n")
        
        return {
            "question": request.question,
            "answer": result.get("answer", ""),
            "sources": result.get("sources", []),
            "confidence": result.get("confidence", 0.0),
            "recommendation": result.get("recommendation", "HOLD"),
            "reasoning_path": result.get("reasoning_path", []),
            "subtasks": result.get("subtasks", []),
            "iteration_count": result.get("iteration_count", 0),
            "processing_steps": result.get("processing_steps", []),
            "mode": "AGENTIC_WORKFLOW",
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ Agentic Workflow ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Agentic Workflow ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"
        )

# @app.post("/query")ëŠ” "ì§ˆë¬¸ì„ ë°›ì•„ì„œ ë‹µë³€ì„ ì£¼ëŠ”" ì—”ë“œí¬ì¸íŠ¸ì˜ˆìš”!
# mode íŒŒë¼ë¯¸í„°ë¡œ "api" ë˜ëŠ” "local"ì„ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!
@app.post("/query",
          summary="ì§ˆë¬¸-ë‹µë³€",
          description="GraphRAGì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ì•„ìš”. modeë¡œ 'api' ë˜ëŠ” 'local'ì„ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!\n\n**ìš”ì²­ í˜•ì‹**:\n```json\n{\n  \"question\": \"ì§ˆë¬¸ ë‚´ìš©\",\n  \"mode\": \"local\"\n}\n```",
          response_description="ì§ˆë¬¸ê³¼ ë‹µë³€",
          responses={
              200: {
                  "description": "ì§ˆë¬¸ ì„±ê³µ",
                  "content": {
                      "application/json": {
                          "example": {
                              "question": "What is NVIDIA revenue?",
                              "answer": "NVIDIA's revenue is $57.0 billion in Q3 2026.",
                              "mode": "local",
                              "status": "success"
                          }
                      }
                  }
              },
              422: {
                  "description": "ìš”ì²­ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜",
                  "content": {
                      "application/json": {
                          "example": {
                              "detail": [
                                  {
                                      "type": "missing",
                                      "loc": ["body", "question"],
                                      "msg": "Field required",
                                      "input": {}
                                  }
                              ]
                          }
                      }
                  }
              }
          })
async def query(request: QueryRequest):
    # ifëŠ” "ë§Œì•½"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
    if engine is None:
        raise HTTPException(status_code=503, detail="ì—”ì§„ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì–´ìš”!")
    
    # question í•„ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if not request.question or not request.question.strip():
        raise HTTPException(
            status_code=422,
            detail="'question' í•„ë“œëŠ” ë¹„ì–´ìˆì„ ìˆ˜ ì—†ì–´ìš”! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )
    
    # modeê°€ "api" ë˜ëŠ” "local"ì´ ì•„ë‹ˆë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œìš”!
    if request.mode not in ["api", "local"]:
        raise HTTPException(
            status_code=400,
            detail="modeëŠ” 'api' ë˜ëŠ” 'local'ì´ì–´ì•¼ í•´ìš”! (í˜„ì¬ ê°’: '{}')".format(request.mode)
        )
    
    try:
        # #region agent log
        import json
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:325","message":"Query entry","data":{"question":request.question,"mode":request.mode,"enable_web_search":request.enable_web_search},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H2,H5"})+'\n')
        # #endregion
        
        # --- Decision Layer (Router) ---
        # ì›¹ ê²€ìƒ‰ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ ë¶„ë¥˜
        if request.enable_web_search:
            # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ë¥˜ (GRAPH_RAG vs WEB_SEARCH)
            print(f"ğŸ¤” ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ (ì›¹ ê²€ìƒ‰ í™œì„±í™”ë¨): '{request.question}'")
            query_type = await classify_query(request.question)
            print(f"âœ… ë¶„ë¥˜ ê²°ê³¼: {query_type}")
        else:
            # ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™” ì‹œ í•­ìƒ GraphRAG ì‚¬ìš©
            query_type = "GRAPH_RAG"
            print(f"ğŸ“š ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™” - ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤")
        
        # 2ë‹¨ê³„: ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ì²˜ë¦¬
        sources_list = []
        
        if query_type == "WEB_SEARCH":
            # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬ - Multi-Agent ëª¨ë“œ ì‚¬ìš© ê¶Œì¥
            print(f"ğŸŒ ì›¹ ê²€ìƒ‰ ëª¨ë“œ ê°ì§€ - Multi-Agent ëª¨ë“œ ê¶Œì¥")
            response = "ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì€ Multi-Agent Analysis ëª¨ë“œì—ì„œ ë” ê°•ë ¥í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤. Advanced Settingsì—ì„œ 'Multi-Agent Analysis Mode'ë¥¼ í™œì„±í™”í•œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            sources_list = []
            source = "WEB_SEARCH"
        else:
            # GraphRAGë¡œ ì²˜ë¦¬ (ì¶œì²˜ ì •ë³´ í¬í•¨)
            print(f"ğŸ“š GraphRAG ëª¨ë“œë¡œ ì²˜ë¦¬ (mode: {request.mode}, search_type: {request.search_type}, temperature: {request.temperature}, top_k: {request.top_k})")
            retrieval_backend = "unknown"
            retrieval_context = ""
            
            # Global vs Local search ë¶„ê¸°
            if request.search_type == "global":
                # Global Search: ì „ì²´ ë¬¸ì„œ ìš”ì•½
                result = await engine.aglobal_search(
                    request.question,
                    top_k=request.top_k,
                    temperature=request.temperature
                )
                base_answer = result.get("answer", "")
                sources_list = result.get("sources", [])
                retrieval_backend = "community"
            else:
                # Local Search: íŠ¹ì • ì—”í‹°í‹° ê²€ìƒ‰
                result = await engine.aquery(
                    request.question,
                    mode=request.mode,
                    return_context=True,
                    top_k=request.top_k
                )
                
                if isinstance(result, dict):
                    base_answer = result.get("answer", "")
                    sources_list = result.get("sources", [])
                    retrieval_backend = result.get("retrieval_backend", "unknown")
                    retrieval_context = result.get("context", "") or ""
                else:
                    base_answer = result
                    sources_list = []
            
            # Strict Groundingìœ¼ë¡œ ë³´ê³ ì„œ ì¬ìƒì„±
            if sources_list:
                # #region agent log
                with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"app.py:567","message":"sources_list before grounding","data":{"count":len(sources_list),"sources":[{"id":s.get("id"),"file":s.get("file"),"excerpt":s.get("excerpt","")[:100]} for s in sources_list[:3]]},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H4"})+'\n')
                # #endregion
                # ì‹¤ì œ ì†ŒìŠ¤ ê°œìˆ˜ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì œí•œ
                max_sources = min(len(sources_list), 10)  # ìµœëŒ€ 10ê°œ
                sources_list = sources_list[:max_sources]
                
                # Strict Grounding Prompt ì‚¬ìš©
                from utils import get_strict_grounding_prompt
                strict_prompt = get_strict_grounding_prompt(request.question, sources_list)
                
                client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
                llm_response = await client.chat.completions.create(
                    model=ROUTER_MODEL,
                    messages=[
                        {"role": "system", "content": strict_prompt},
                        {"role": "user", "content": request.question}
                    ],
                    temperature=0.0,  # Strict grounding: ì°½ì˜ì„± ì œê±°
                    max_tokens=2000
                )
                response = llm_response.choices[0].message.content.strip()
                # #region agent log
                with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"app.py:584","message":"LLM response before validation","data":{"response_length":len(response),"response_preview":response[:300]},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H5"})+'\n')
                # #endregion
                
                # Self-Correction: Citation Validation
                from citation_validator import CitationValidator
                validator = CitationValidator(sources_list)
                validation_result = validator.validate_response(response)
                evidence = []
                # #region agent log
                with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                    f.write(__import__('json').dumps({"location":"app.py:596","message":"validation result","data":{"confidence":validation_result.get('confidence_score'),"valid_citations":validation_result.get('valid_citations'),"total_citations":validation_result.get('total_citations'),"missing_citations":validation_result.get('missing_citations',[])},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H5"})+'\n')
                # #endregion
                
                print(f"[VALIDATION] Confidence: {validation_result['confidence_score']:.1%}")
                print(f"[VALIDATION] Valid citations: {validation_result['valid_citations']}/{validation_result['total_citations']}")

                # Strict Grounding LLMì´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ë‹µí–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ì†ŒìŠ¤ê°€ ì¶©ë¶„í•œ ê²½ìš° ë³´ì •
                override_applied = False
                if response.strip() == "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." and len(sources_list) > 0:
                    print("[WARNING] Strict grounding LLM returned 'no info' despite non-empty sources. Falling back to base GraphRAG answer.")
                    # #region agent log
                    with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
                        f.write(__import__('json').dumps({
                            "location": "app.py:610",
                            "message": "override no-info with base_answer",
                            "data": {
                                "base_answer_preview": base_answer[:200] if base_answer else None,
                                "sources_count": len(sources_list)
                            },
                            "timestamp": __import__('time').time()*1000,
                            "sessionId": "debug-session",
                            "runId": "run1",
                            "hypothesisId": "H4,H5"
                        }) + '\n')
                    # #endregion
                    response = base_answer or response
                    # base_answerì—ëŠ” citationì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ validationì€ ìœ ì§€í•˜ë˜ ì‹ ë¢°ë„ëŠ” 0.7 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í›„ì† ì²´í¬ë¥¼ í†µê³¼ì‹œí‚´
                    validation_result = {"confidence_score": 0.75, "is_valid": True}
                    evidence = []
                    override_applied = True

                # ì‹ ë¢°ë„ê°€ ë‚®ê±°ë‚˜ ì‘ë‹µì´ ë¹„ì •ìƒì ì´ë©´ Perplexityë¡œ í´ë°±
                # ë˜ëŠ” ì‘ë‹µì— HTML/ì›¹ ê²€ìƒ‰ í”ì ì´ ìˆìœ¼ë©´ ê±°ë¶€
                # ë‹¨, overrideê°€ ì ìš©ëœ ê²½ìš°ëŠ” ìŠ¤í‚µ (base_answerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ)
                if not override_applied and (validation_result["confidence_score"] < 0.7 or 
                    "<a href" in response or 
                    "Thesaurus.com" in response or
                    "WordHippo" in response or
                    len(response.strip()) < 50):
                    print(f"[WARNING] Low confidence or invalid response, falling back to Perplexity search")
                    
                    # Perplexityë¡œ í´ë°±
                    try:
                        from engine.search_handler import SearchHandler
                        search_handler = SearchHandler()
                        
                        # ì§ˆë¬¸ì—ì„œ ê³µê°œ ì—”í‹°í‹° ì¶”ì¶œ
                        perplexity_result = search_handler.search(
                            query=request.question,
                            max_results=5,
                            sanitize=True
                        )
                        
                        if perplexity_result and not perplexity_result.get("error"):
                            # Perplexity ë‹µë³€ ì‚¬ìš©
                            response = f"## ì‹¤ì‹œê°„ ê²€ìƒ‰ ê²°ê³¼ (Perplexity)\n\n{perplexity_result.get('answer', '')}"
                            
                            # Citationsë¥¼ sourcesë¡œ ë³€í™˜
                            sources_list = []
                            for i, url in enumerate(perplexity_result.get('citations', [])[:5], 1):
                                sources_list.append({
                                    'id': i,
                                    'file': 'Perplexity Web Search',
                                    'url': url,
                                    'excerpt': f"ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ #{i}"
                                })
                            
                            validation_result = {"confidence_score": 0.8, "is_valid": True}
                            evidence = []
                            print(f"âœ… Perplexity fallback successful: {len(sources_list)} sources")
                        else:
                            # Perplexityë„ ì‹¤íŒ¨í•œ ê²½ìš°
                            response = "ë°ì´í„°ë² ì´ìŠ¤ì™€ ì‹¤ì‹œê°„ ê²€ìƒ‰ ëª¨ë‘ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            sources_list = []
                            validation_result = {"confidence_score": 0.0, "is_valid": False}
                            evidence = []
                    except Exception as e:
                        print(f"âŒ Perplexity fallback failed: {e}")
                        response = "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        sources_list = []
                        validation_result = {"confidence_score": 0.0, "is_valid": False}
                        evidence = []
                else:
                    # ì‘ë‹µì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ëœ citation ë²ˆí˜¸ ì¶”ì¶œ ë° í•„í„°ë§
                    import re
                    citation_pattern = r'\[(\d+)\]'
                    used_citations = set()
                    for match in re.finditer(citation_pattern, response):
                        citation_num = int(match.group(1))
                        if 1 <= citation_num <= len(sources_list):
                            used_citations.add(citation_num)
                    
                    # ì‚¬ìš©ëœ citationì— í•´ë‹¹í•˜ëŠ” ì†ŒìŠ¤ë§Œ ìœ ì§€
                    if used_citations:
                        sources_list = [s for s in sources_list if s['id'] in used_citations]
                        # IDë¥¼ 1ë¶€í„° ë‹¤ì‹œ ë§¤í•‘
                        for idx, source in enumerate(sources_list, 1):
                            old_id = source['id']
                            source['id'] = idx
                            # ì‘ë‹µì—ì„œ citation ë²ˆí˜¸ ì¬ë§¤í•‘
                            response = response.replace(f'[{old_id}]', f'[{idx}]')
                            response = re.sub(rf'\[{old_id}\]', f'[{idx}]', response)

                    # evidence(í´ë ˆì„-ê·¼ê±°) êµ¬ì¡° ìƒì„± (citation remap ì´í›„)
                    # overrideê°€ ì ìš©ëœ ê²½ìš°ëŠ” evidenceë¥¼ ë¹ˆ ë°°ì—´ë¡œ ìœ ì§€ (base_answerì—ëŠ” citationì´ ì—†ìŒ)
                    if not override_applied:
                        validator = CitationValidator(sources_list)
                        evidence = validator.build_evidence(response)
                    # override_appliedì¸ ê²½ìš° evidenceëŠ” ì´ë¯¸ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •ë¨
            else:
                # ì¶œì²˜ê°€ ì—†ìœ¼ë©´ Perplexityë¡œ í´ë°±
                print(f"ğŸ“š No sources found in database, falling back to Perplexity search")
                
                try:
                    from engine.search_handler import SearchHandler
                    search_handler = SearchHandler()
                    
                    # Perplexity ê²€ìƒ‰
                    perplexity_result = search_handler.search(
                        query=request.question,
                        max_results=5,
                        sanitize=True
                    )
                    
                    if perplexity_result and not perplexity_result.get("error"):
                        # Perplexity ë‹µë³€ ì‚¬ìš©
                        response = f"## ì‹¤ì‹œê°„ ê²€ìƒ‰ ê²°ê³¼ (Perplexity)\n\n{perplexity_result.get('answer', '')}"
                        
                        # Citationsë¥¼ sourcesë¡œ ë³€í™˜
                        sources_list = []
                        for i, url in enumerate(perplexity_result.get('citations', [])[:5], 1):
                            sources_list.append({
                                'id': i,
                                'file': 'Perplexity Web Search',
                                'url': url,
                                'excerpt': f"ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ #{i}"
                            })
                        
                        validation_result = {"confidence_score": 0.8, "is_valid": True}
                        evidence = []
                        print(f"âœ… Perplexity fallback successful: {len(sources_list)} sources")
                    else:
                        response = "ë°ì´í„°ë² ì´ìŠ¤ì™€ ì‹¤ì‹œê°„ ê²€ìƒ‰ ëª¨ë‘ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        validation_result = {"confidence_score": 0.0, "is_valid": False}
                        evidence = []
                except Exception as e:
                    print(f"âŒ Perplexity fallback failed: {e}")
                    response = "í•´ë‹¹ ë¬¸ì„œë“¤ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    validation_result = {"confidence_score": 0.0, "is_valid": False}
                    evidence = []
            
            source = "GRAPH_RAG"
        
        # #region agent log
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:338","message":"Query response","data":{"response":response[:500] if response else None,"response_type":type(response).__name__,"source":source},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H3"})+'\n')
        # #endregion
        
        # returnì€ "ì´ê±¸ ëŒë ¤ì¤˜"ë¼ëŠ” ëœ»ì´ì—ìš”!
        return {
            "question": request.question,
            "answer": response,
            "sources": sources_list,  # Citationìš© ì¶œì²˜ ë¦¬ìŠ¤íŠ¸
            "source": source,  # ì–´ë””ì„œ ë‹µë³€ì„ ê°€ì ¸ì™”ëŠ”ì§€ ì•Œë ¤ì¤˜ìš”!
            "mode": request.mode if source == "GRAPH_RAG" else "N/A",  # GraphRAGì¼ ë•Œë§Œ ì˜ë¯¸ ìˆì–´ìš”
            "search_type": request.search_type if source == "GRAPH_RAG" else "N/A",
            "validation": validation_result if source == "GRAPH_RAG" and 'validation_result' in locals() else None,
            "evidence": evidence if source == "GRAPH_RAG" and 'evidence' in locals() else [],
            "retrieval_backend": retrieval_backend if source == "GRAPH_RAG" and 'retrieval_backend' in locals() else "N/A",
            "retrieval_context": retrieval_context if source == "GRAPH_RAG" and 'retrieval_context' in locals() else "",
            "status": "success"
        }
    except Exception as e:
        # #region agent log
        import traceback
        with open('/Users/gyuteoi/Desktop/Finance_GraphRAG/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"location":"app.py:352","message":"Query error","data":{"error":str(e),"error_type":type(e).__name__,"traceback":traceback.format_exc()},"timestamp":__import__('time').time()*1000,"sessionId":"debug-session","runId":"run1","hypothesisId":"H1,H4"})+'\n')
        # #endregion
        # exceptëŠ” "ë§Œì•½ ì—ëŸ¬ê°€ ìƒê¸°ë©´"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”: {str(e)}")


# --- [12] PDF Upload Endpoint (Local Model) ---
@app.post("/ingest_pdf",
          summary="PDF Upload and Processing (Local)",
          description="Upload PDF document and extract graph with Local Ollama model")
async def ingest_pdf(file: UploadFile = File(...)):
    """
    Upload and process PDF with Local Ollama model
    
    Process:
    - Extract text from PDF (PyMuPDF)
    - Extract entities + relationships with Ollama (qwen2.5-coder)
    - Merge into Neo4j graph database
    """
    import tempfile
    from pathlib import Path
    import json
    
    try:
        # Validate file type
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400,
                detail="Only PDF files are supported"
            )
        
        print(f"ğŸ“„ Received PDF upload: {file.filename}")
        
        # Save uploaded file to temp location
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_path = tmp_file.name
        
        print(f"ğŸ’¾ Saved to: {tmp_path}")

        # Extract text with PyMuPDF
        try:
            import pymupdf
            doc = pymupdf.open(tmp_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"PDF text extraction failed: {str(e)}")

        if not text or len(text.strip()) < 10:
            raise HTTPException(status_code=400, detail="PDF contains no extractable text")

        print(f"âœ… Extracted {len(text)} characters from PDF")

        # Extract entities + relationships with Local Ollama
        from engine.extractor import KnowledgeExtractor
        
        extractor = KnowledgeExtractor()
        
        chunk_size = 1000  # 500 â†’ 1000ìœ¼ë¡œ ì¦ê°€ (ìš”ì²­ íšŸìˆ˜ 50% ê°ì†Œ)
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        
        # ì²­í¬ ìˆ˜ ì œí•œ (ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•)
        max_chunks = 20  # ìµœëŒ€ 20ê°œ ì²­í¬ (ì•½ 20,000ì, ì²­í¬ í¬ê¸° ì¦ê°€ë¡œ ì»¤ë²„ëŸ‰ ìœ ì§€)
        if len(chunks) > max_chunks:
            print(f"âš ï¸ ì²­í¬ ìˆ˜ ì œí•œ: {len(chunks)} â†’ {max_chunks} (ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•)")
            chunks = chunks[:max_chunks]
        
        all_entities = []
        all_relationships = []

        print(f"ğŸ”’ Processing {len(chunks)} chunks with Local Ollama (parallel)...")

        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ (ë™ì‹œì— 3ê°œì”© ì²˜ë¦¬)
        import asyncio
        batch_size = 3
        
        for batch_start in range(0, len(chunks), batch_size):
            batch = chunks[batch_start:batch_start + batch_size]
            print(f"   Progress: {batch_start}/{len(chunks)} chunks ({batch_start*100//len(chunks)}%)")
            
            # ë°°ì¹˜ ë‚´ ì²­í¬ë“¤ì„ ë™ì‹œ ì²˜ë¦¬
            tasks = [extractor.extract_entities(chunk) for chunk in batch]
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        print(f"âš ï¸ Chunk {batch_start + i} failed: {result}")
                        continue
                    all_entities.extend(result.get("entities", []))
                    all_relationships.extend(result.get("relationships", []))
            except Exception as e:
                print(f"âš ï¸ Batch processing failed: {e}")
                continue

        print(f"âœ… Extracted {len(all_entities)} entities, {len(all_relationships)} relationships")

        graph_data = {
            "entities": all_entities,
            "relationships": all_relationships
        }

        # Store in Neo4j via integrator
        from engine.integrator import DataIntegrator

        if NEO4J_URI and NEO4J_PASSWORD:
            integrator = DataIntegrator()
            merge_stats = integrator.ingestPdfGraph(
                graphData=graph_data,
                sourceFile=file.filename,
                sourceLabel=Path(file.filename).stem
            )
            integrator.close()
            print("âœ… PDF graph merge complete")
        else:
            merge_stats = {"entitiesMerged": 0, "relationshipsCreated": 0}
            print("âš ï¸ Neo4j not configured, skipping storage")
        
        # Clean up temp file
        Path(tmp_path).unlink()
        
        return {
            "message": "PDF processed with Local Ollama and merged into Neo4j",
            "filename": file.filename,
            "text_length": len(text),
            "entities_extracted": len(all_entities),
            "relationships_extracted": len(all_relationships),
            "merge_stats": merge_stats,
            "processing_model": "ollama-local",
            "status": "success"
        }
        
    except Exception as e:
        droneLogError("PDF processing failed", e)
        import traceback
        print(f"âŒ PDF processing error: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"PDF processing failed: {str(e)}"
        )


# --- [13] PDF Upload for Database (OpenAI API) ---
@app.post("/ingest_pdf_db",
          summary="PDF Upload for Database (OpenAI API)",
          description="Upload PDF document and extract graph with OpenAI API for permanent database storage")
async def ingest_pdf_db(file: UploadFile = File(...)):
    """
    Upload and process PDF with OpenAI API for database merge
    
    Process:
    - Extract text from PDF (PyMuPDF)
    - Extract entities + relationships with GPT-4o-mini
    - Merge into Neo4j graph database permanently
    """
    import tempfile
    from pathlib import Path
    import json
    
    try:
        # Validate file type
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400,
                detail="Only PDF files are supported"
            )

        print(f"ğŸ“„ Received PDF upload for DB: {file.filename}")

        # Save uploaded file to temp location
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_path = tmp_file.name

        print(f"ğŸ’¾ Saved to: {tmp_path}")

        # Extract text with PyMuPDF
        try:
            import pymupdf
            doc = pymupdf.open(tmp_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"PDF text extraction failed: {str(e)}")

        if not text or len(text.strip()) < 10:
            raise HTTPException(status_code=400, detail="PDF contains no extractable text")

        print(f"âœ… Extracted {len(text)} characters from PDF")

        # Extract entities + relationships with OpenAI API (GPT-4o-mini)
        from openai import AsyncOpenAI
        
        client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
        
        chunk_size = 2000  # Larger chunks for OpenAI
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        all_entities = []
        all_relationships = []

        print(f"ğŸ¤– Processing {len(chunks)} chunks with GPT-4o-mini...")

        for i, chunk in enumerate(chunks):
            if i > 0 and i % 10 == 0:
                print(f"   Progress: {i}/{len(chunks)} chunks")
            
            prompt = f"""Extract business entities and relationships from this text.
Return ONLY valid JSON format:

{{
  "entities": [
    {{"name": "EntityName", "type": "COMPANY|PERSON|PRODUCT|TECHNOLOGY|FINANCIAL_METRIC|LOCATION", "properties": {{"key": "value"}}}}
  ],
  "relationships": [
    {{"source": "EntityA", "target": "EntityB", "type": "RELATIONSHIP_TYPE", "properties": {{"key": "value"}}}}
  ]
}}

Entity types: COMPANY, PERSON, PRODUCT, TECHNOLOGY, FINANCIAL_METRIC, LOCATION, REGULATION, RISK
Relationship types: SUPPLIES, PURCHASES, COMPETES_WITH, HAS_CEO, EMPLOYS, LOCATED_IN, PRODUCES, IMPACTS, DEPENDS_ON

Text:
{chunk[:2000]}

JSON output:"""

            try:
                response = await client.chat.completions.create(
                    model=ROUTER_MODEL,  # gpt-4o-mini
                    messages=[
                        {"role": "system", "content": "You are a financial document analyzer. Extract structured entities and relationships. Respond with valid JSON only."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=2000
                )
                
                content = response.choices[0].message.content.strip()
                
                # Parse JSON from response
                if content.startswith("```json"):
                    content = content[7:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()
                
                extracted = json.loads(content)
                all_entities.extend(extracted.get("entities", []))
                all_relationships.extend(extracted.get("relationships", []))
                
            except Exception as e:
                print(f"âš ï¸ Chunk {i} extraction failed: {e}")
                continue

        print(f"âœ… Extracted {len(all_entities)} entities, {len(all_relationships)} relationships")

        graph_data = {
            "entities": all_entities,
            "relationships": all_relationships
        }

        # Store in Neo4j via integrator
        from engine.integrator import DataIntegrator

        if NEO4J_URI and NEO4J_PASSWORD:
            integrator = DataIntegrator()
            merge_stats = integrator.ingestPdfGraph(
                graphData=graph_data,
                sourceFile=file.filename,
                sourceLabel=Path(file.filename).stem
            )
            integrator.close()
            print("âœ… PDF graph merge complete (OpenAI)")
        else:
            merge_stats = {"entitiesMerged": 0, "relationshipsCreated": 0}
            print("âš ï¸ Neo4j not configured, skipping storage")

        # Clean up temp file
        Path(tmp_path).unlink()

        return {
            "message": "PDF processed with OpenAI API and merged into Neo4j database",
            "filename": file.filename,
            "text_length": len(text),
            "entities_extracted": len(all_entities),
            "relationships_extracted": len(all_relationships),
            "merge_stats": merge_stats,
            "processing_model": "gpt-4o-mini",
            "status": "success"
        }

    except Exception as e:
        droneLogError("PDF DB processing failed", e)
        import traceback
        print(f"âŒ PDF DB processing error: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"PDF processing failed: {str(e)}"
        )


# --- [14] CSV Upload Endpoint (REMOVED) ---
# CSV/JSON uploads have been removed per user request

# Old CSV endpoint
@app.post("/upload_csv_old",
          summary="CSV Data Upload to Neo4j",
          description="Upload CSV data directly to Neo4j graph database")
async def upload_csv(request: dict):
    """
    CSV ë°ì´í„°ë¥¼ Neo4jì— ì§ì ‘ ì—…ë¡œë“œ (ë¡œì»¬ ì²˜ë¦¬ë§Œ)
    """
    try:
        data = request.get("data", [])
        entity_column = request.get("entity_column")
        entity_type = request.get("entity_type", "Entity")
        property_columns = request.get("property_columns", [])
        
        if not data or not entity_column:
            raise HTTPException(status_code=400, detail="data and entity_column are required")
        
        print(f"ğŸ“Š Uploading CSV data: {len(data)} rows (ë¡œì»¬ ì²˜ë¦¬)")
        
        # Neo4jì— ë°ì´í„° ì‚½ì…
        from db.neo4j_db import Neo4jDatabase
        
        if not NEO4J_URI or not NEO4J_PASSWORD:
            raise HTTPException(status_code=500, detail="Neo4j not configured")
        
        db = Neo4jDatabase(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)
        
        nodes_created = 0
        
        for row in data:
            entity_name = row.get(entity_column)
            if not entity_name:
                continue
            
            # ë…¸ë“œ ì†ì„± ì¤€ë¹„
            properties = {col: row.get(col) for col in property_columns if col in row}
            properties['name'] = entity_name
            
            # Cypher ì¿¼ë¦¬ ìƒì„±
            query = f"MERGE (n:{entity_type} {{name: $name}}) SET n += $properties RETURN n"
            
            db.execute_query(query, {"name": entity_name, "properties": properties})
            nodes_created += 1
        
        db.close()
        
        return {
            "message": f"Successfully uploaded {nodes_created} nodes (ë¡œì»¬ ì²˜ë¦¬)",
            "nodes_created": nodes_created,
            "relationships_created": 0,
            "status": "success"
        }
    
    except Exception as e:
        print(f"âŒ CSV upload failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# --- [14] JSON Upload Endpoint ---
@app.post("/upload_json",
          summary="JSON Data Upload to Neo4j",
          description="Upload JSON data directly to Neo4j graph database")
async def upload_json(request: dict):
    """
    JSON ë°ì´í„°ë¥¼ Neo4jì— ì§ì ‘ ì—…ë¡œë“œ (ë¡œì»¬ ì²˜ë¦¬ë§Œ)
    """
    try:
        data = request.get("data")
        root_key = request.get("root_key")
        entity_key = request.get("entity_key", "name")
        entity_type = request.get("entity_type", "Entity")
        
        if not data:
            raise HTTPException(status_code=400, detail="data is required")
        
        # ë£¨íŠ¸ í‚¤ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë°°ì—´ ì¶”ì¶œ
        if root_key and isinstance(data, dict):
            data = data.get(root_key, [])
        
        # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if not isinstance(data, list):
            data = [data]
        
        print(f"ğŸ“¦ Uploading JSON data: {len(data)} items (ë¡œì»¬ ì²˜ë¦¬)")
        
        # Neo4jì— ë°ì´í„° ì‚½ì…
        from db.neo4j_db import Neo4jDatabase
        
        if not NEO4J_URI or not NEO4J_PASSWORD:
            raise HTTPException(status_code=500, detail="Neo4j not configured")
        
        db = Neo4jDatabase(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)
        
        nodes_created = 0
        
        for item in data:
            if not isinstance(item, dict):
                continue
            
            entity_name = item.get(entity_key)
            if not entity_name:
                continue
            
            # ëª¨ë“  ì†ì„± í¬í•¨
            properties = dict(item)
            properties['name'] = entity_name
            
            # Cypher ì¿¼ë¦¬ ìƒì„±
            query = f"MERGE (n:{entity_type} {{name: $name}}) SET n += $properties RETURN n"
            
            db.execute_query(query, {"name": entity_name, "properties": properties})
            nodes_created += 1
        
        db.close()
        
        return {
            "message": f"Successfully uploaded {nodes_created} nodes (ë¡œì»¬ ì²˜ë¦¬)",
            "nodes_created": nodes_created,
            "relationships_created": 0,
            "status": "success"
        }
    
    except Exception as e:
        print(f"âŒ JSON upload failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# --- [15] ì„œë²„ ì‹¤í–‰ ---
# if __name__ == "__main__": ì´ê±´ "ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ"ì´ë¼ëŠ” ëœ»ì´ì—ìš”!
if __name__ == "__main__":
    # uvicorn.run()ì€ "ì„œë²„ë¥¼ ì‹¤í–‰í•˜ëŠ”" ê±°ì˜ˆìš”!
    # appì€ "FastAPI ì•±"ì´ì—ìš”!
    # host="0.0.0.0"ì€ "ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ì† ê°€ëŠ¥"í•˜ë‹¤ëŠ” ëœ»ì´ì—ìš”!
    # port=8000ì€ "8000ë²ˆ í¬íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤"ëŠ” ëœ»ì´ì—ìš”!
    uvicorn.run(app, host="0.0.0.0", port=8000)

